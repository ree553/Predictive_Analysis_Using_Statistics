{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('Creditcard_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (763, 31)\n",
      "class 1: (9, 31)\n"
     ]
    }
   ],
   "source": [
    "#Class count and Separate Class\n",
    "class_count_0, class_count_1 = dataset['Class'].value_counts()\n",
    "class_0 = dataset[dataset['Class'] == 0]\n",
    "class_1 = dataset[dataset['Class'] == 1]\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class of 1 and0: 0    9\n",
      "1    9\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_0_under = class_0.sample(class_count_1)\n",
    "\n",
    "test_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "\n",
    "print(\"total class of 1 and0:\",test_under['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (9, 31)\n",
      "class 1: (9, 31)\n"
     ]
    }
   ],
   "source": [
    "#UnderSampling\n",
    "# class count\n",
    "class_count_0, class_count_1 = dataset['Class'].value_counts()\n",
    "\n",
    "# Separate class\n",
    "class_0 = test_under[test_under['Class'] == 0]\n",
    "class_1 = test_under[test_under['Class'] == 1]# print the shape of the class\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class of 1 and 0: 0    9\n",
      "1    9\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: title={'center': 'count (target)'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_1_over = class_1.sample(class_count_0, replace=True)\n",
    "\n",
    "test_over = pd.concat([class_1_over, class_0], axis=0)\n",
    "\n",
    "print(\"total class of 1 and 0:\",test_under['Class'].value_counts())\n",
    "test_over['Class'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (763, 31)\n",
      "class 1: (763, 31)\n"
     ]
    }
   ],
   "source": [
    "#Oversampling\n",
    "# class count\n",
    "class_count_0, class_count_1 = dataset['Class'].value_counts()\n",
    "\n",
    "# Separate class\n",
    "class_0 = test_over[test_over['Class'] == 0]\n",
    "class_1 = test_over[test_over['Class'] == 1]\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Split dataset into input features and labels\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "\n",
    "# Apply Random Over Sampling to balance the dataset\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Save the balanced dataset to a new CSV file\n",
    "balanced_data = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "balanced_data.to_csv('balanced_creditcard.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.1599999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=1.96\n",
    "p=0.5\n",
    "e=0.05\n",
    "n = (z**2*p*(1-p))/(e**2)\n",
    "n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Techniques\n",
    "1.Random Sampling\n",
    "2.Systematic Sampling\n",
    "3.Cluster Sampling\n",
    "4.Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for random sampling:  308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>49</td>\n",
       "      <td>-0.549626</td>\n",
       "      <td>0.418949</td>\n",
       "      <td>1.729833</td>\n",
       "      <td>0.203065</td>\n",
       "      <td>-0.187012</td>\n",
       "      <td>0.253878</td>\n",
       "      <td>0.500894</td>\n",
       "      <td>0.251256</td>\n",
       "      <td>-0.227985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115062</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>-0.065133</td>\n",
       "      <td>0.264981</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.395969</td>\n",
       "      <td>0.027182</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>59.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>118</td>\n",
       "      <td>1.254914</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.302488</td>\n",
       "      <td>0.693114</td>\n",
       "      <td>-0.371470</td>\n",
       "      <td>-1.070256</td>\n",
       "      <td>0.086781</td>\n",
       "      <td>-0.202836</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287592</td>\n",
       "      <td>-0.832682</td>\n",
       "      <td>0.128083</td>\n",
       "      <td>0.339427</td>\n",
       "      <td>0.215944</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>-0.023354</td>\n",
       "      <td>0.030892</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>499</td>\n",
       "      <td>1.255439</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>-0.428876</td>\n",
       "      <td>-1.088456</td>\n",
       "      <td>0.043840</td>\n",
       "      <td>-0.167739</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294795</td>\n",
       "      <td>-0.882126</td>\n",
       "      <td>0.136846</td>\n",
       "      <td>0.327949</td>\n",
       "      <td>0.194459</td>\n",
       "      <td>0.096516</td>\n",
       "      <td>-0.027271</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>539</td>\n",
       "      <td>-1.738582</td>\n",
       "      <td>0.052740</td>\n",
       "      <td>1.187057</td>\n",
       "      <td>-0.656652</td>\n",
       "      <td>0.920623</td>\n",
       "      <td>-0.291788</td>\n",
       "      <td>0.269083</td>\n",
       "      <td>0.140631</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179545</td>\n",
       "      <td>-0.192036</td>\n",
       "      <td>-0.261879</td>\n",
       "      <td>-0.237477</td>\n",
       "      <td>-0.335040</td>\n",
       "      <td>0.240323</td>\n",
       "      <td>-0.345129</td>\n",
       "      <td>-0.383563</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>155</td>\n",
       "      <td>1.171954</td>\n",
       "      <td>0.311213</td>\n",
       "      <td>0.313605</td>\n",
       "      <td>0.519230</td>\n",
       "      <td>-0.058032</td>\n",
       "      <td>-0.258769</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>0.039599</td>\n",
       "      <td>-0.344227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200153</td>\n",
       "      <td>-0.541916</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>-0.001739</td>\n",
       "      <td>0.139121</td>\n",
       "      <td>0.104376</td>\n",
       "      <td>-0.005414</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>484</td>\n",
       "      <td>-0.928088</td>\n",
       "      <td>0.398194</td>\n",
       "      <td>1.741131</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>-0.901004</td>\n",
       "      <td>0.879016</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>-0.142117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066353</td>\n",
       "      <td>0.281378</td>\n",
       "      <td>-0.257966</td>\n",
       "      <td>0.385384</td>\n",
       "      <td>0.391117</td>\n",
       "      <td>-0.453853</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.125765</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>130</td>\n",
       "      <td>-0.485238</td>\n",
       "      <td>0.658497</td>\n",
       "      <td>1.949967</td>\n",
       "      <td>1.249695</td>\n",
       "      <td>0.426410</td>\n",
       "      <td>0.231513</td>\n",
       "      <td>0.585115</td>\n",
       "      <td>0.029163</td>\n",
       "      <td>-0.520297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>-0.232563</td>\n",
       "      <td>0.225572</td>\n",
       "      <td>0.025892</td>\n",
       "      <td>-0.247395</td>\n",
       "      <td>-0.025381</td>\n",
       "      <td>-0.118565</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>561</td>\n",
       "      <td>1.214872</td>\n",
       "      <td>0.307970</td>\n",
       "      <td>0.278629</td>\n",
       "      <td>0.642981</td>\n",
       "      <td>-0.185161</td>\n",
       "      <td>-0.603602</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>-0.064239</td>\n",
       "      <td>0.030872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260682</td>\n",
       "      <td>-0.716148</td>\n",
       "      <td>0.159804</td>\n",
       "      <td>0.033516</td>\n",
       "      <td>0.143170</td>\n",
       "      <td>0.124259</td>\n",
       "      <td>-0.006506</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "1439   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "76      49 -0.549626  0.418949  1.729833  0.203065 -0.187012  0.253878   \n",
       "1010   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
       "660    499  1.255439  0.307729  0.292700  0.699873 -0.428876 -1.088456   \n",
       "1132   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "855    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "233    155  1.171954  0.311213  0.313605  0.519230 -0.058032 -0.258769   \n",
       "1475   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
       "196    130 -0.485238  0.658497  1.949967  1.249695  0.426410  0.231513   \n",
       "752    561  1.214872  0.307970  0.278629  0.642981 -0.185161 -0.603602   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "1439  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
       "76    0.500894  0.251256 -0.227985  ...  0.115062  0.418529 -0.065133   \n",
       "1010  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
       "660   0.043840 -0.167739  0.128854  ... -0.294795 -0.882126  0.136846   \n",
       "1132  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "855   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "233  -0.043843  0.039599 -0.344227  ... -0.200153 -0.541916  0.137491   \n",
       "1475  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
       "196   0.585115  0.029163 -0.520297  ...  0.007290  0.328244 -0.232563   \n",
       "752   0.006881 -0.064239  0.030872  ... -0.260682 -0.716148  0.159804   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "1439 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n",
       "76    0.264981  0.003958  0.395969  0.027182  0.043506   59.99      0  \n",
       "1010  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
       "660   0.327949  0.194459  0.096516 -0.027271  0.029491    1.98      0  \n",
       "1132 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "855  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "233  -0.001739  0.139121  0.104376 -0.005414  0.018728    1.29      0  \n",
       "1475  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
       "196   0.225572  0.025892 -0.247395 -0.025381 -0.118565    5.97      0  \n",
       "752   0.033516  0.143170  0.124259 -0.006506  0.027226    0.89      0  \n",
       "\n",
       "[308 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling no 1- Random Sampling\n",
    "import math\n",
    "\n",
    "\n",
    "confidence_level = 0.95\n",
    "margin_of_error = 0.05\n",
    "\n",
    "\n",
    "balanced_data = pd.read_csv('balanced_creditcard.csv')\n",
    "total_population = balanced_data.shape[0]\n",
    "\n",
    "\n",
    "sample_size = (1.96**2 * 0.5 * 0.5 * total_population) / ((margin_of_error**2 * (total_population - 1)) + (1.96**2 * 0.5 * 0.5))\n",
    "\n",
    "\n",
    "sample_size1 = math.ceil(sample_size)\n",
    "\n",
    "\n",
    "print(\"Sample size for random sampling: \", sample_size1)\n",
    "\n",
    "Sample1 = balanced_data.sample(n=sample_size1, random_state=42)\n",
    "\n",
    "Sample1.to_csv('creditcard_random_sample.csv', index=False)\n",
    "\n",
    "Sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0       0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "27     23  1.322707 -0.174041  0.434555  0.576038 -0.836758 -0.831083   \n",
      "54     37  1.295668  0.341483  0.081505  0.566746 -0.110459 -0.766325   \n",
      "81     52  1.147369  0.059035  0.263632  1.211023 -0.044096  0.301067   \n",
      "108    73  1.162281  1.248178 -1.581317  1.475024  1.138357 -1.020373   \n",
      "\n",
      "           V7        V8        V9  ...       V21       V22       V23  \\\n",
      "0    0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
      "27  -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357 -0.037710   \n",
      "54   0.073155 -0.168304  0.071837  ... -0.323607 -0.929781  0.063809   \n",
      "81  -0.132960  0.227885  0.252191  ... -0.087813 -0.110756 -0.097771   \n",
      "108  0.638387 -0.136762 -0.805505  ... -0.124012 -0.227150 -0.199185   \n",
      "\n",
      "          V24       V25       V26       V27       V28  Amount  Class  \n",
      "0    0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "27   0.347151  0.559639 -0.280158  0.042335  0.028822   16.00      0  \n",
      "54  -0.193565  0.287574  0.127881 -0.023731  0.025200    0.99      0  \n",
      "81  -0.323374  0.633279 -0.305328  0.027394 -0.000580    6.67      0  \n",
      "108 -0.289757  0.776244 -0.283950  0.056747  0.084706    1.00      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#Sampling no 2- Systematic Sampling\n",
    "import math\n",
    "n = len(dataset)\n",
    "k = int(math.sqrt(n))\n",
    "Sample2 = balanced_data.iloc[::k]\n",
    "print(Sample2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "961    164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "963    406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
      "724    548 -1.233426 -0.212441  1.839632 -1.802986 -0.493195  0.350424   \n",
      "134     83 -1.897331  0.955626  0.052543  1.276656 -3.323084  3.229911   \n",
      "983      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "...    ...       ...       ...       ...       ...       ...       ...   \n",
      "1143   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "450    328 -4.236419 -4.459784  1.381813  1.117080  6.044486 -3.498447   \n",
      "1151     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "874    164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "504    370  1.354445 -0.815297  0.836498 -0.617140 -1.304124 -0.025274   \n",
      "\n",
      "            V7        V8        V9  ...       V22       V23       V24  \\\n",
      "961   0.296319  0.139497 -0.123050  ... -0.381932  0.151012 -1.363967   \n",
      "963  -2.537387  1.391657 -2.770089  ... -0.035049 -0.465211  0.320198   \n",
      "724  -0.905316  0.844863 -1.523517  ...  1.574750 -0.176482 -0.221561   \n",
      "134   1.029631  1.515607 -0.059627  ...  0.776078  0.477537 -0.608981   \n",
      "983  -0.078803  0.085102 -0.255425  ... -0.638672  0.101288 -0.339846   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1143  0.094272 -0.210300  0.014455  ... -0.820658  0.127663  0.343128   \n",
      "450  -2.740892  0.372155 -0.214338  ... -0.432517  0.612507 -1.016362   \n",
      "1151 -0.078803  0.085102 -0.255425  ... -0.638672  0.101288 -0.339846   \n",
      "874   0.296319  0.139497 -0.123050  ... -0.381932  0.151012 -1.363967   \n",
      "504  -1.147177  0.162996 -0.258341  ...  0.941357 -0.192851  0.049422   \n",
      "\n",
      "           V25       V26       V27       V28  Amount  Class  cluster  \n",
      "961  -1.389079  0.075412  0.231750  0.230171    0.99      1        0  \n",
      "963   0.044519  0.177840  0.261145 -0.143276    0.00      1        0  \n",
      "724  -0.058504 -0.163971  0.014670 -0.033210   24.99      0        0  \n",
      "134  -1.120892 -0.413851  0.061399 -0.187964  552.18      0        0  \n",
      "983   0.167170  0.125895 -0.008983  0.014724    2.69      1        0  \n",
      "...        ...       ...       ...       ...     ...    ...      ...  \n",
      "1143  0.221120  0.094391 -0.022189  0.030944    1.29      1        1  \n",
      "450   0.630373 -0.498141 -0.094774  0.208038    2.50      0        1  \n",
      "1151  0.167170  0.125895 -0.008983  0.014724    2.69      1        1  \n",
      "874  -1.389079  0.075412  0.231750  0.230171    0.99      1        1  \n",
      "504   0.597416  0.002149  0.035182  0.003682    4.79      0        1  \n",
      "\n",
      "[1200 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "#Sampling no 3- Cluster Sampling\n",
    "def get_clustered_Sample(df, n_per_cluster, num_select_clusters):\n",
    "    N = len(df)\n",
    "    K = int(N/n_per_cluster)\n",
    "    data = None\n",
    "    for k in range(K):\n",
    "        sample_k = df.sample(n_per_cluster)\n",
    "        sample_k[\"cluster\"] = np.repeat(k,len(sample_k))\n",
    "        df = df.drop(index = sample_k.index)\n",
    "        data = pd.concat([data,sample_k],axis = 0)\n",
    "\n",
    "    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)\n",
    "    samples = data[data.cluster.isin(random_chosen_clusters)]\n",
    "    return(samples)\n",
    "\n",
    "Sample3 = get_clustered_Sample(df = balanced_data, n_per_cluster = 600, num_select_clusters = 2)\n",
    "print(Sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>332</td>\n",
       "      <td>1.084303</td>\n",
       "      <td>0.127678</td>\n",
       "      <td>1.389853</td>\n",
       "      <td>2.532559</td>\n",
       "      <td>-0.636871</td>\n",
       "      <td>0.651109</td>\n",
       "      <td>-0.685289</td>\n",
       "      <td>0.356924</td>\n",
       "      <td>-0.052520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055487</td>\n",
       "      <td>-0.088642</td>\n",
       "      <td>-0.012251</td>\n",
       "      <td>-0.026491</td>\n",
       "      <td>0.290882</td>\n",
       "      <td>-0.039353</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>225</td>\n",
       "      <td>-0.608831</td>\n",
       "      <td>0.876837</td>\n",
       "      <td>2.495715</td>\n",
       "      <td>3.138674</td>\n",
       "      <td>0.161264</td>\n",
       "      <td>-0.107099</td>\n",
       "      <td>0.515854</td>\n",
       "      <td>-0.138226</td>\n",
       "      <td>-1.035070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262866</td>\n",
       "      <td>-0.439237</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>0.717471</td>\n",
       "      <td>0.199742</td>\n",
       "      <td>0.072401</td>\n",
       "      <td>0.099756</td>\n",
       "      <td>-0.136524</td>\n",
       "      <td>35.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>509</td>\n",
       "      <td>-0.152397</td>\n",
       "      <td>-0.748114</td>\n",
       "      <td>1.659571</td>\n",
       "      <td>-2.160601</td>\n",
       "      <td>-1.448594</td>\n",
       "      <td>0.558854</td>\n",
       "      <td>-0.274352</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>-1.964295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.474039</td>\n",
       "      <td>-0.084319</td>\n",
       "      <td>-0.353695</td>\n",
       "      <td>-0.226044</td>\n",
       "      <td>-0.143999</td>\n",
       "      <td>-0.173635</td>\n",
       "      <td>-0.241563</td>\n",
       "      <td>114.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>222</td>\n",
       "      <td>1.086971</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.475774</td>\n",
       "      <td>1.344244</td>\n",
       "      <td>-0.302641</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>-0.096404</td>\n",
       "      <td>0.182898</td>\n",
       "      <td>0.193464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044044</td>\n",
       "      <td>-0.007869</td>\n",
       "      <td>-0.044243</td>\n",
       "      <td>0.213380</td>\n",
       "      <td>0.562625</td>\n",
       "      <td>-0.340026</td>\n",
       "      <td>0.026737</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>18.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>519</td>\n",
       "      <td>0.764614</td>\n",
       "      <td>1.706191</td>\n",
       "      <td>-1.755823</td>\n",
       "      <td>1.557386</td>\n",
       "      <td>1.101083</td>\n",
       "      <td>-1.529455</td>\n",
       "      <td>0.917702</td>\n",
       "      <td>-0.190132</td>\n",
       "      <td>-0.748935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085310</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>-0.459204</td>\n",
       "      <td>-0.421570</td>\n",
       "      <td>0.260012</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>574</td>\n",
       "      <td>1.257719</td>\n",
       "      <td>0.364739</td>\n",
       "      <td>0.306923</td>\n",
       "      <td>0.690638</td>\n",
       "      <td>-0.357792</td>\n",
       "      <td>-1.067481</td>\n",
       "      <td>0.094272</td>\n",
       "      <td>-0.210300</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286856</td>\n",
       "      <td>-0.820658</td>\n",
       "      <td>0.127663</td>\n",
       "      <td>0.343128</td>\n",
       "      <td>0.221120</td>\n",
       "      <td>0.094391</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>484</td>\n",
       "      <td>-0.928088</td>\n",
       "      <td>0.398194</td>\n",
       "      <td>1.741131</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>-0.901004</td>\n",
       "      <td>0.879016</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>-0.142117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066353</td>\n",
       "      <td>0.281378</td>\n",
       "      <td>-0.257966</td>\n",
       "      <td>0.385384</td>\n",
       "      <td>0.391117</td>\n",
       "      <td>-0.453853</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.125765</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "454    332  1.084303  0.127678  1.389853  2.532559 -0.636871  0.651109   \n",
       "313    225 -0.608831  0.876837  2.495715  3.138674  0.161264 -0.107099   \n",
       "670    509 -0.152397 -0.748114  1.659571 -2.160601 -1.448594  0.558854   \n",
       "309    222  1.086971  0.045122  0.475774  1.344244 -0.302641  0.004408   \n",
       "688    519  0.764614  1.706191 -1.755823  1.557386  1.101083 -1.529455   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1354     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "973    574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
       "1094   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
       "1123   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "1298   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "454  -0.685289  0.356924 -0.052520  ... -0.055487 -0.088642 -0.012251   \n",
       "313   0.515854 -0.138226 -1.035070  ... -0.262866 -0.439237 -0.006775   \n",
       "670  -0.274352  0.010159 -1.964295  ...  0.004382  0.474039 -0.084319   \n",
       "309  -0.096404  0.182898  0.193464  ... -0.044044 -0.007869 -0.044243   \n",
       "688   0.917702 -0.190132 -0.748935  ... -0.085310  0.022452  0.056012   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1354 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "973   0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
       "1094  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
       "1123  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "1298  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "454  -0.026491  0.290882 -0.039353  0.033400  0.022966   11.34      0  \n",
       "313   0.717471  0.199742  0.072401  0.099756 -0.136524   35.11      0  \n",
       "670  -0.353695 -0.226044 -0.143999 -0.173635 -0.241563  114.00      0  \n",
       "309   0.213380  0.562625 -0.340026  0.026737  0.007185   18.61      0  \n",
       "688   0.008466 -0.459204 -0.421570  0.260012  0.006571    0.89      0  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "1354 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
       "973   0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
       "1094  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
       "1123 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "1298 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n",
       "\n",
       "[600 rows x 31 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling no 4-Stratified Sampling\n",
    "Sample4 = balanced_data.groupby('Class', group_keys=False).apply(lambda x:x.sample(300))\n",
    "Sample4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0\n",
      "90.0\n",
      "63.0\n",
      "97.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample1.drop('Class', axis=1)\n",
    "y = Sample1['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking accuracy of different machine learning models using various sampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.0\n",
      "56.00000000000001\n",
      "53.0\n",
      "80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample2.drop('Class', axis=1)\n",
    "y = Sample2['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n",
      "97.0\n",
      "71.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample3.drop('Class', axis=1)\n",
    "y = Sample3['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0\n",
      "94.0\n",
      "69.0\n",
      "98.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample4.drop('Class', axis=1)\n",
    "y = Sample4['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:- We can see that Decision Tree Classifier gives 99% accuracy using Cluster Sampling Technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b98f472bb8ba48098397e3b897b5be76f7bf0e62d98845cdb0e8066dc5677259"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
