{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('Creditcard_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (763, 31)\n",
      "class 1: (9, 31)\n"
     ]
    }
   ],
   "source": [
    "#Class count and Separate Class\n",
    "class_count_0, class_count_1 = dataset['Class'].value_counts()\n",
    "class_0 = dataset[dataset['Class'] == 0]\n",
    "class_1 = dataset[dataset['Class'] == 1]\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class of 1 and0: 0    9\n",
      "1    9\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_0_under = class_0.sample(class_count_1)\n",
    "\n",
    "test_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "\n",
    "print(\"total class of 1 and0:\",test_under['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (9, 31)\n",
      "class 1: (9, 31)\n"
     ]
    }
   ],
   "source": [
    "#UnderSampling\n",
    "# class count\n",
    "class_count_0, class_count_1 = dataset['Class'].value_counts()\n",
    "\n",
    "# Separate class\n",
    "class_0 = test_under[test_under['Class'] == 0]\n",
    "class_1 = test_under[test_under['Class'] == 1]# print the shape of the class\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class of 1 and 0: 0    9\n",
      "1    9\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: title={'center': 'count (target)'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_1_over = class_1.sample(class_count_0, replace=True)\n",
    "\n",
    "test_over = pd.concat([class_1_over, class_0], axis=0)\n",
    "\n",
    "print(\"total class of 1 and 0:\",test_under['Class'].value_counts())\n",
    "test_over['Class'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (9, 31)\n",
      "class 1: (763, 31)\n"
     ]
    }
   ],
   "source": [
    "#Oversampling\n",
    "# class count\n",
    "class_count_0, class_count_1 = dataset['Class'].value_counts()\n",
    "\n",
    "# Separate class\n",
    "class_0 = test_over[test_over['Class'] == 0]\n",
    "class_1 = test_over[test_over['Class'] == 1]\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Split dataset into input features and labels\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "\n",
    "# Apply Random Over Sampling to balance the dataset\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Save the balanced dataset to a new CSV file\n",
    "balanced_data = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "balanced_data.to_csv('balanced_creditcard.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.1599999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=1.96\n",
    "p=0.5\n",
    "e=0.05\n",
    "n = (z**2*p*(1-p))/(e**2)\n",
    "n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Techniques\n",
    "1.Random Sampling\n",
    "2.Systematic Sampling\n",
    "3.Cluster Sampling\n",
    "4.Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for random sampling:  308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>49</td>\n",
       "      <td>-0.549626</td>\n",
       "      <td>0.418949</td>\n",
       "      <td>1.729833</td>\n",
       "      <td>0.203065</td>\n",
       "      <td>-0.187012</td>\n",
       "      <td>0.253878</td>\n",
       "      <td>0.500894</td>\n",
       "      <td>0.251256</td>\n",
       "      <td>-0.227985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115062</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>-0.065133</td>\n",
       "      <td>0.264981</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.395969</td>\n",
       "      <td>0.027182</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>59.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>118</td>\n",
       "      <td>1.254914</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.302488</td>\n",
       "      <td>0.693114</td>\n",
       "      <td>-0.371470</td>\n",
       "      <td>-1.070256</td>\n",
       "      <td>0.086781</td>\n",
       "      <td>-0.202836</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287592</td>\n",
       "      <td>-0.832682</td>\n",
       "      <td>0.128083</td>\n",
       "      <td>0.339427</td>\n",
       "      <td>0.215944</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>-0.023354</td>\n",
       "      <td>0.030892</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>499</td>\n",
       "      <td>1.255439</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>-0.428876</td>\n",
       "      <td>-1.088456</td>\n",
       "      <td>0.043840</td>\n",
       "      <td>-0.167739</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294795</td>\n",
       "      <td>-0.882126</td>\n",
       "      <td>0.136846</td>\n",
       "      <td>0.327949</td>\n",
       "      <td>0.194459</td>\n",
       "      <td>0.096516</td>\n",
       "      <td>-0.027271</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>539</td>\n",
       "      <td>-1.738582</td>\n",
       "      <td>0.052740</td>\n",
       "      <td>1.187057</td>\n",
       "      <td>-0.656652</td>\n",
       "      <td>0.920623</td>\n",
       "      <td>-0.291788</td>\n",
       "      <td>0.269083</td>\n",
       "      <td>0.140631</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179545</td>\n",
       "      <td>-0.192036</td>\n",
       "      <td>-0.261879</td>\n",
       "      <td>-0.237477</td>\n",
       "      <td>-0.335040</td>\n",
       "      <td>0.240323</td>\n",
       "      <td>-0.345129</td>\n",
       "      <td>-0.383563</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>155</td>\n",
       "      <td>1.171954</td>\n",
       "      <td>0.311213</td>\n",
       "      <td>0.313605</td>\n",
       "      <td>0.519230</td>\n",
       "      <td>-0.058032</td>\n",
       "      <td>-0.258769</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>0.039599</td>\n",
       "      <td>-0.344227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200153</td>\n",
       "      <td>-0.541916</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>-0.001739</td>\n",
       "      <td>0.139121</td>\n",
       "      <td>0.104376</td>\n",
       "      <td>-0.005414</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>484</td>\n",
       "      <td>-0.928088</td>\n",
       "      <td>0.398194</td>\n",
       "      <td>1.741131</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>-0.901004</td>\n",
       "      <td>0.879016</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>-0.142117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066353</td>\n",
       "      <td>0.281378</td>\n",
       "      <td>-0.257966</td>\n",
       "      <td>0.385384</td>\n",
       "      <td>0.391117</td>\n",
       "      <td>-0.453853</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.125765</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>130</td>\n",
       "      <td>-0.485238</td>\n",
       "      <td>0.658497</td>\n",
       "      <td>1.949967</td>\n",
       "      <td>1.249695</td>\n",
       "      <td>0.426410</td>\n",
       "      <td>0.231513</td>\n",
       "      <td>0.585115</td>\n",
       "      <td>0.029163</td>\n",
       "      <td>-0.520297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>-0.232563</td>\n",
       "      <td>0.225572</td>\n",
       "      <td>0.025892</td>\n",
       "      <td>-0.247395</td>\n",
       "      <td>-0.025381</td>\n",
       "      <td>-0.118565</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>561</td>\n",
       "      <td>1.214872</td>\n",
       "      <td>0.307970</td>\n",
       "      <td>0.278629</td>\n",
       "      <td>0.642981</td>\n",
       "      <td>-0.185161</td>\n",
       "      <td>-0.603602</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>-0.064239</td>\n",
       "      <td>0.030872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260682</td>\n",
       "      <td>-0.716148</td>\n",
       "      <td>0.159804</td>\n",
       "      <td>0.033516</td>\n",
       "      <td>0.143170</td>\n",
       "      <td>0.124259</td>\n",
       "      <td>-0.006506</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "1439   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "76      49 -0.549626  0.418949  1.729833  0.203065 -0.187012  0.253878   \n",
       "1010   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
       "660    499  1.255439  0.307729  0.292700  0.699873 -0.428876 -1.088456   \n",
       "1132   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "855    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "233    155  1.171954  0.311213  0.313605  0.519230 -0.058032 -0.258769   \n",
       "1475   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
       "196    130 -0.485238  0.658497  1.949967  1.249695  0.426410  0.231513   \n",
       "752    561  1.214872  0.307970  0.278629  0.642981 -0.185161 -0.603602   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "1439  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
       "76    0.500894  0.251256 -0.227985  ...  0.115062  0.418529 -0.065133   \n",
       "1010  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
       "660   0.043840 -0.167739  0.128854  ... -0.294795 -0.882126  0.136846   \n",
       "1132  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "855   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "233  -0.043843  0.039599 -0.344227  ... -0.200153 -0.541916  0.137491   \n",
       "1475  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
       "196   0.585115  0.029163 -0.520297  ...  0.007290  0.328244 -0.232563   \n",
       "752   0.006881 -0.064239  0.030872  ... -0.260682 -0.716148  0.159804   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "1439 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n",
       "76    0.264981  0.003958  0.395969  0.027182  0.043506   59.99      0  \n",
       "1010  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
       "660   0.327949  0.194459  0.096516 -0.027271  0.029491    1.98      0  \n",
       "1132 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "855  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "233  -0.001739  0.139121  0.104376 -0.005414  0.018728    1.29      0  \n",
       "1475  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
       "196   0.225572  0.025892 -0.247395 -0.025381 -0.118565    5.97      0  \n",
       "752   0.033516  0.143170  0.124259 -0.006506  0.027226    0.89      0  \n",
       "\n",
       "[308 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling no 1- Random Sampling\n",
    "import math\n",
    "\n",
    "\n",
    "confidence_level = 0.95\n",
    "margin_of_error = 0.05\n",
    "\n",
    "\n",
    "balanced_data = pd.read_csv('balanced_creditcard.csv')\n",
    "total_population = balanced_data.shape[0]\n",
    "\n",
    "\n",
    "sample_size = (1.96**2 * 0.5 * 0.5 * total_population) / ((margin_of_error**2 * (total_population - 1)) + (1.96**2 * 0.5 * 0.5))\n",
    "\n",
    "\n",
    "sample_size1 = math.ceil(sample_size)\n",
    "\n",
    "\n",
    "print(\"Sample size for random sampling: \", sample_size1)\n",
    "\n",
    "Sample1 = balanced_data.sample(n=sample_size1, random_state=42)\n",
    "\n",
    "Sample1.to_csv('creditcard_random_sample.csv', index=False)\n",
    "\n",
    "Sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0       0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "27     23  1.322707 -0.174041  0.434555  0.576038 -0.836758 -0.831083   \n",
      "54     37  1.295668  0.341483  0.081505  0.566746 -0.110459 -0.766325   \n",
      "81     52  1.147369  0.059035  0.263632  1.211023 -0.044096  0.301067   \n",
      "108    73  1.162281  1.248178 -1.581317  1.475024  1.138357 -1.020373   \n",
      "\n",
      "           V7        V8        V9  ...       V21       V22       V23  \\\n",
      "0    0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
      "27  -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357 -0.037710   \n",
      "54   0.073155 -0.168304  0.071837  ... -0.323607 -0.929781  0.063809   \n",
      "81  -0.132960  0.227885  0.252191  ... -0.087813 -0.110756 -0.097771   \n",
      "108  0.638387 -0.136762 -0.805505  ... -0.124012 -0.227150 -0.199185   \n",
      "\n",
      "          V24       V25       V26       V27       V28  Amount  Class  \n",
      "0    0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "27   0.347151  0.559639 -0.280158  0.042335  0.028822   16.00      0  \n",
      "54  -0.193565  0.287574  0.127881 -0.023731  0.025200    0.99      0  \n",
      "81  -0.323374  0.633279 -0.305328  0.027394 -0.000580    6.67      0  \n",
      "108 -0.289757  0.776244 -0.283950  0.056747  0.084706    1.00      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#Sampling no 2- Systematic Sampling\n",
    "import math\n",
    "n = len(dataset)\n",
    "k = int(math.sqrt(n))\n",
    "Sample2 = balanced_data.iloc[::k]\n",
    "print(Sample2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "961    164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "963    406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
      "724    548 -1.233426 -0.212441  1.839632 -1.802986 -0.493195  0.350424   \n",
      "134     83 -1.897331  0.955626  0.052543  1.276656 -3.323084  3.229911   \n",
      "983      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "...    ...       ...       ...       ...       ...       ...       ...   \n",
      "1143   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "450    328 -4.236419 -4.459784  1.381813  1.117080  6.044486 -3.498447   \n",
      "1151     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "874    164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "504    370  1.354445 -0.815297  0.836498 -0.617140 -1.304124 -0.025274   \n",
      "\n",
      "            V7        V8        V9  ...       V22       V23       V24  \\\n",
      "961   0.296319  0.139497 -0.123050  ... -0.381932  0.151012 -1.363967   \n",
      "963  -2.537387  1.391657 -2.770089  ... -0.035049 -0.465211  0.320198   \n",
      "724  -0.905316  0.844863 -1.523517  ...  1.574750 -0.176482 -0.221561   \n",
      "134   1.029631  1.515607 -0.059627  ...  0.776078  0.477537 -0.608981   \n",
      "983  -0.078803  0.085102 -0.255425  ... -0.638672  0.101288 -0.339846   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1143  0.094272 -0.210300  0.014455  ... -0.820658  0.127663  0.343128   \n",
      "450  -2.740892  0.372155 -0.214338  ... -0.432517  0.612507 -1.016362   \n",
      "1151 -0.078803  0.085102 -0.255425  ... -0.638672  0.101288 -0.339846   \n",
      "874   0.296319  0.139497 -0.123050  ... -0.381932  0.151012 -1.363967   \n",
      "504  -1.147177  0.162996 -0.258341  ...  0.941357 -0.192851  0.049422   \n",
      "\n",
      "           V25       V26       V27       V28  Amount  Class  cluster  \n",
      "961  -1.389079  0.075412  0.231750  0.230171    0.99      1        0  \n",
      "963   0.044519  0.177840  0.261145 -0.143276    0.00      1        0  \n",
      "724  -0.058504 -0.163971  0.014670 -0.033210   24.99      0        0  \n",
      "134  -1.120892 -0.413851  0.061399 -0.187964  552.18      0        0  \n",
      "983   0.167170  0.125895 -0.008983  0.014724    2.69      1        0  \n",
      "...        ...       ...       ...       ...     ...    ...      ...  \n",
      "1143  0.221120  0.094391 -0.022189  0.030944    1.29      1        1  \n",
      "450   0.630373 -0.498141 -0.094774  0.208038    2.50      0        1  \n",
      "1151  0.167170  0.125895 -0.008983  0.014724    2.69      1        1  \n",
      "874  -1.389079  0.075412  0.231750  0.230171    0.99      1        1  \n",
      "504   0.597416  0.002149  0.035182  0.003682    4.79      0        1  \n",
      "\n",
      "[1200 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "#Sampling no 3- Cluster Sampling\n",
    "def get_clustered_Sample(df, n_per_cluster, num_select_clusters):\n",
    "    N = len(df)\n",
    "    K = int(N/n_per_cluster)\n",
    "    data = None\n",
    "    for k in range(K):\n",
    "        sample_k = df.sample(n_per_cluster)\n",
    "        sample_k[\"cluster\"] = np.repeat(k,len(sample_k))\n",
    "        df = df.drop(index = sample_k.index)\n",
    "        data = pd.concat([data,sample_k],axis = 0)\n",
    "\n",
    "    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)\n",
    "    samples = data[data.cluster.isin(random_chosen_clusters)]\n",
    "    return(samples)\n",
    "\n",
    "Sample3 = get_clustered_Sample(df = balanced_data, n_per_cluster = 600, num_select_clusters = 2)\n",
    "print(Sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>332</td>\n",
       "      <td>1.084303</td>\n",
       "      <td>0.127678</td>\n",
       "      <td>1.389853</td>\n",
       "      <td>2.532559</td>\n",
       "      <td>-0.636871</td>\n",
       "      <td>0.651109</td>\n",
       "      <td>-0.685289</td>\n",
       "      <td>0.356924</td>\n",
       "      <td>-0.052520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055487</td>\n",
       "      <td>-0.088642</td>\n",
       "      <td>-0.012251</td>\n",
       "      <td>-0.026491</td>\n",
       "      <td>0.290882</td>\n",
       "      <td>-0.039353</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>225</td>\n",
       "      <td>-0.608831</td>\n",
       "      <td>0.876837</td>\n",
       "      <td>2.495715</td>\n",
       "      <td>3.138674</td>\n",
       "      <td>0.161264</td>\n",
       "      <td>-0.107099</td>\n",
       "      <td>0.515854</td>\n",
       "      <td>-0.138226</td>\n",
       "      <td>-1.035070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262866</td>\n",
       "      <td>-0.439237</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>0.717471</td>\n",
       "      <td>0.199742</td>\n",
       "      <td>0.072401</td>\n",
       "      <td>0.099756</td>\n",
       "      <td>-0.136524</td>\n",
       "      <td>35.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>509</td>\n",
       "      <td>-0.152397</td>\n",
       "      <td>-0.748114</td>\n",
       "      <td>1.659571</td>\n",
       "      <td>-2.160601</td>\n",
       "      <td>-1.448594</td>\n",
       "      <td>0.558854</td>\n",
       "      <td>-0.274352</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>-1.964295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.474039</td>\n",
       "      <td>-0.084319</td>\n",
       "      <td>-0.353695</td>\n",
       "      <td>-0.226044</td>\n",
       "      <td>-0.143999</td>\n",
       "      <td>-0.173635</td>\n",
       "      <td>-0.241563</td>\n",
       "      <td>114.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>222</td>\n",
       "      <td>1.086971</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.475774</td>\n",
       "      <td>1.344244</td>\n",
       "      <td>-0.302641</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>-0.096404</td>\n",
       "      <td>0.182898</td>\n",
       "      <td>0.193464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044044</td>\n",
       "      <td>-0.007869</td>\n",
       "      <td>-0.044243</td>\n",
       "      <td>0.213380</td>\n",
       "      <td>0.562625</td>\n",
       "      <td>-0.340026</td>\n",
       "      <td>0.026737</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>18.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>519</td>\n",
       "      <td>0.764614</td>\n",
       "      <td>1.706191</td>\n",
       "      <td>-1.755823</td>\n",
       "      <td>1.557386</td>\n",
       "      <td>1.101083</td>\n",
       "      <td>-1.529455</td>\n",
       "      <td>0.917702</td>\n",
       "      <td>-0.190132</td>\n",
       "      <td>-0.748935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085310</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>-0.459204</td>\n",
       "      <td>-0.421570</td>\n",
       "      <td>0.260012</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>574</td>\n",
       "      <td>1.257719</td>\n",
       "      <td>0.364739</td>\n",
       "      <td>0.306923</td>\n",
       "      <td>0.690638</td>\n",
       "      <td>-0.357792</td>\n",
       "      <td>-1.067481</td>\n",
       "      <td>0.094272</td>\n",
       "      <td>-0.210300</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286856</td>\n",
       "      <td>-0.820658</td>\n",
       "      <td>0.127663</td>\n",
       "      <td>0.343128</td>\n",
       "      <td>0.221120</td>\n",
       "      <td>0.094391</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>484</td>\n",
       "      <td>-0.928088</td>\n",
       "      <td>0.398194</td>\n",
       "      <td>1.741131</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>-0.901004</td>\n",
       "      <td>0.879016</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>-0.142117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066353</td>\n",
       "      <td>0.281378</td>\n",
       "      <td>-0.257966</td>\n",
       "      <td>0.385384</td>\n",
       "      <td>0.391117</td>\n",
       "      <td>-0.453853</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.125765</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "454    332  1.084303  0.127678  1.389853  2.532559 -0.636871  0.651109   \n",
       "313    225 -0.608831  0.876837  2.495715  3.138674  0.161264 -0.107099   \n",
       "670    509 -0.152397 -0.748114  1.659571 -2.160601 -1.448594  0.558854   \n",
       "309    222  1.086971  0.045122  0.475774  1.344244 -0.302641  0.004408   \n",
       "688    519  0.764614  1.706191 -1.755823  1.557386  1.101083 -1.529455   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1354     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "973    574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
       "1094   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
       "1123   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "1298   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "454  -0.685289  0.356924 -0.052520  ... -0.055487 -0.088642 -0.012251   \n",
       "313   0.515854 -0.138226 -1.035070  ... -0.262866 -0.439237 -0.006775   \n",
       "670  -0.274352  0.010159 -1.964295  ...  0.004382  0.474039 -0.084319   \n",
       "309  -0.096404  0.182898  0.193464  ... -0.044044 -0.007869 -0.044243   \n",
       "688   0.917702 -0.190132 -0.748935  ... -0.085310  0.022452  0.056012   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1354 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "973   0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
       "1094  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
       "1123  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "1298  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "454  -0.026491  0.290882 -0.039353  0.033400  0.022966   11.34      0  \n",
       "313   0.717471  0.199742  0.072401  0.099756 -0.136524   35.11      0  \n",
       "670  -0.353695 -0.226044 -0.143999 -0.173635 -0.241563  114.00      0  \n",
       "309   0.213380  0.562625 -0.340026  0.026737  0.007185   18.61      0  \n",
       "688   0.008466 -0.459204 -0.421570  0.260012  0.006571    0.89      0  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "1354 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
       "973   0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
       "1094  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
       "1123 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "1298 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n",
       "\n",
       "[600 rows x 31 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling no 4-Stratified Sampling\n",
    "Sample4 = balanced_data.groupby('Class', group_keys=False).apply(lambda x:x.sample(300))\n",
    "Sample4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>156</td>\n",
       "      <td>-3.494861</td>\n",
       "      <td>-2.894450</td>\n",
       "      <td>1.637989</td>\n",
       "      <td>-0.274976</td>\n",
       "      <td>-0.389203</td>\n",
       "      <td>-0.703275</td>\n",
       "      <td>0.444194</td>\n",
       "      <td>0.154266</td>\n",
       "      <td>0.695818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.676254</td>\n",
       "      <td>0.596377</td>\n",
       "      <td>0.114229</td>\n",
       "      <td>0.834915</td>\n",
       "      <td>0.309675</td>\n",
       "      <td>0.632261</td>\n",
       "      <td>500.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>517</td>\n",
       "      <td>-0.639474</td>\n",
       "      <td>-0.048355</td>\n",
       "      <td>2.452755</td>\n",
       "      <td>0.310804</td>\n",
       "      <td>-0.430963</td>\n",
       "      <td>-0.290032</td>\n",
       "      <td>0.166889</td>\n",
       "      <td>0.006196</td>\n",
       "      <td>0.651675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.110847</td>\n",
       "      <td>0.057008</td>\n",
       "      <td>0.389171</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>0.218186</td>\n",
       "      <td>-0.067664</td>\n",
       "      <td>-0.073760</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>427</td>\n",
       "      <td>-0.847312</td>\n",
       "      <td>0.854261</td>\n",
       "      <td>0.338816</td>\n",
       "      <td>0.890137</td>\n",
       "      <td>0.804751</td>\n",
       "      <td>1.165501</td>\n",
       "      <td>-0.081408</td>\n",
       "      <td>0.879014</td>\n",
       "      <td>-0.394737</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046690</td>\n",
       "      <td>-0.075301</td>\n",
       "      <td>-0.308479</td>\n",
       "      <td>-1.733137</td>\n",
       "      <td>0.087036</td>\n",
       "      <td>-0.129209</td>\n",
       "      <td>0.294334</td>\n",
       "      <td>0.071198</td>\n",
       "      <td>11.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>133</td>\n",
       "      <td>0.709336</td>\n",
       "      <td>-0.011679</td>\n",
       "      <td>-0.530213</td>\n",
       "      <td>0.236331</td>\n",
       "      <td>1.611750</td>\n",
       "      <td>4.091915</td>\n",
       "      <td>-0.862138</td>\n",
       "      <td>1.083973</td>\n",
       "      <td>0.786796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.090033</td>\n",
       "      <td>0.271773</td>\n",
       "      <td>0.986547</td>\n",
       "      <td>-0.809943</td>\n",
       "      <td>-0.529660</td>\n",
       "      <td>-0.065887</td>\n",
       "      <td>-0.125002</td>\n",
       "      <td>35.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>539</td>\n",
       "      <td>-0.420050</td>\n",
       "      <td>1.034648</td>\n",
       "      <td>1.565850</td>\n",
       "      <td>-0.029643</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>-0.907983</td>\n",
       "      <td>0.688436</td>\n",
       "      <td>-0.063128</td>\n",
       "      <td>-0.389635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228282</td>\n",
       "      <td>-0.598231</td>\n",
       "      <td>-0.023081</td>\n",
       "      <td>0.360199</td>\n",
       "      <td>-0.194316</td>\n",
       "      <td>0.071640</td>\n",
       "      <td>0.265159</td>\n",
       "      <td>0.121071</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>529</td>\n",
       "      <td>-2.000567</td>\n",
       "      <td>-2.495484</td>\n",
       "      <td>2.467149</td>\n",
       "      <td>1.140053</td>\n",
       "      <td>2.462010</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>-2.110183</td>\n",
       "      <td>0.788347</td>\n",
       "      <td>0.958809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422452</td>\n",
       "      <td>1.195394</td>\n",
       "      <td>0.297836</td>\n",
       "      <td>-0.857105</td>\n",
       "      <td>-0.219322</td>\n",
       "      <td>0.861019</td>\n",
       "      <td>-0.124622</td>\n",
       "      <td>-0.171060</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>574</td>\n",
       "      <td>1.257719</td>\n",
       "      <td>0.364739</td>\n",
       "      <td>0.306923</td>\n",
       "      <td>0.690638</td>\n",
       "      <td>-0.357792</td>\n",
       "      <td>-1.067481</td>\n",
       "      <td>0.094272</td>\n",
       "      <td>-0.210300</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286856</td>\n",
       "      <td>-0.820658</td>\n",
       "      <td>0.127663</td>\n",
       "      <td>0.343128</td>\n",
       "      <td>0.221120</td>\n",
       "      <td>0.094391</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>118</td>\n",
       "      <td>1.254914</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.302488</td>\n",
       "      <td>0.693114</td>\n",
       "      <td>-0.371470</td>\n",
       "      <td>-1.070256</td>\n",
       "      <td>0.086781</td>\n",
       "      <td>-0.202836</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287592</td>\n",
       "      <td>-0.832682</td>\n",
       "      <td>0.128083</td>\n",
       "      <td>0.339427</td>\n",
       "      <td>0.215944</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>-0.023354</td>\n",
       "      <td>0.030892</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>484</td>\n",
       "      <td>-0.928088</td>\n",
       "      <td>0.398194</td>\n",
       "      <td>1.741131</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>-0.901004</td>\n",
       "      <td>0.879016</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>-0.142117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066353</td>\n",
       "      <td>0.281378</td>\n",
       "      <td>-0.257966</td>\n",
       "      <td>0.385384</td>\n",
       "      <td>0.391117</td>\n",
       "      <td>-0.453853</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.125765</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "235    156 -3.494861 -2.894450  1.637989 -0.274976 -0.389203 -0.703275   \n",
       "685    517 -0.639474 -0.048355  2.452755  0.310804 -0.430963 -0.290032   \n",
       "570    427 -0.847312  0.854261  0.338816  0.890137  0.804751  1.165501   \n",
       "201    133  0.709336 -0.011679 -0.530213  0.236331  1.611750  4.091915   \n",
       "716    539 -0.420050  1.034648  1.565850 -0.029643 -0.054336 -0.907983   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1225     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "1209   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
       "1301   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
       "951    118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n",
       "1422   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "235   0.444194  0.154266  0.695818  ...  0.017010 -0.063521  0.676254   \n",
       "685   0.166889  0.006196  0.651675  ...  0.004189  0.110847  0.057008   \n",
       "570  -0.081408  0.879014 -0.394737  ... -0.046690 -0.075301 -0.308479   \n",
       "201  -0.862138  1.083973  0.786796  ...  0.006775  0.090033  0.271773   \n",
       "716   0.688436 -0.063128 -0.389635  ... -0.228282 -0.598231 -0.023081   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1225 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "1209 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
       "1301  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
       "951   0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n",
       "1422  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "235   0.596377  0.114229  0.834915  0.309675  0.632261  500.00      0  \n",
       "685   0.389171 -0.449642  0.218186 -0.067664 -0.073760   59.90      0  \n",
       "570  -1.733137  0.087036 -0.129209  0.294334  0.071198   11.36      0  \n",
       "201   0.986547 -0.809943 -0.529660 -0.065887 -0.125002   35.97      0  \n",
       "716   0.360199 -0.194316  0.071640  0.265159  0.121071    1.29      0  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "1225 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
       "1209 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
       "1301  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
       "951   0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n",
       "1422  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
       "\n",
       "[1200 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling no 5-Stratified Random Sampling\n",
    "Sample5 = balanced_data.groupby('Class', group_keys=False).apply(lambda x:x.sample(600))\n",
    "Sample5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0\n",
      "90.0\n",
      "63.0\n",
      "97.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample1.drop('Class', axis=1)\n",
    "y = Sample1['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking accuracy of different machine learning models using various sampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.0\n",
      "56.00000000000001\n",
      "53.0\n",
      "80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample2.drop('Class', axis=1)\n",
    "y = Sample2['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n",
      "97.0\n",
      "71.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample3.drop('Class', axis=1)\n",
    "y = Sample3['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0\n",
      "94.0\n",
      "69.0\n",
      "98.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample4.drop('Class', axis=1)\n",
    "y = Sample4['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.0\n",
      "97.0\n",
      "70.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "X = Sample5.drop('Class', axis=1)\n",
    "y = Sample5['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(round(training_score.mean(),2)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:- We can see that Decision Tree Classifier gives 99% accuracy using Cluster Sampling Technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b98f472bb8ba48098397e3b897b5be76f7bf0e62d98845cdb0e8066dc5677259"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
